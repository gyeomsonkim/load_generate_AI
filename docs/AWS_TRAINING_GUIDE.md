# AWS í´ë¼ìš°ë“œ í•™ìŠµ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

M1 Macì—ì„œ í•™ìŠµì´ ëŠë¦¬ê±°ë‚˜ ë¦¬ì†ŒìŠ¤ê°€ ë¶€ì¡±í•œ ê²½ìš° AWSì—ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

## ğŸš€ ë°©ë²• 1: AWS EC2 GPU ì¸ìŠ¤í„´ìŠ¤ (ì¶”ì²œ)

### ğŸ’° ë¹„ìš© íš¨ìœ¨ì ì¸ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì²œ

| ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | GPU | vCPU | ë©”ëª¨ë¦¬ | ì‹œê°„ë‹¹ ë¹„ìš© | ì¶”ì²œ ìš©ë„ |
|--------------|-----|------|--------|------------|----------|
| **g4dn.xlarge** | NVIDIA T4 (16GB) | 4 | 16GB | $0.526 | ê°€ì¥ ì €ë ´, í…ŒìŠ¤íŠ¸ |
| **g4dn.2xlarge** | NVIDIA T4 (16GB) | 8 | 32GB | $0.752 | **ì¶”ì²œ**, ê· í˜• |
| **g5.xlarge** | NVIDIA A10G (24GB) | 4 | 16GB | $1.006 | ë¹ ë¥¸ í•™ìŠµ |
| **p3.2xlarge** | NVIDIA V100 (16GB) | 8 | 61GB | $3.06 | ìµœê³  ì„±ëŠ¥ |

**ì¶”ì²œ**: `g4dn.2xlarge` - ì„±ëŠ¥ê³¼ ê°€ê²© ê· í˜•ì´ ì¢‹ìŒ

### 1ï¸âƒ£ EC2 ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘

#### AWS ì½˜ì†”ì—ì„œ ì„¤ì •

```bash
# 1. AWS Management Console ì ‘ì†
# 2. EC2 â†’ Launch Instance
# 3. ì„¤ì •:
#    - Name: ml-training-instance
#    - AMI: Deep Learning AMI GPU PyTorch (Ubuntu 20.04)
#    - Instance type: g4dn.2xlarge
#    - Key pair: ìƒˆë¡œ ìƒì„± ë˜ëŠ” ê¸°ì¡´ ì‚¬ìš©
#    - Storage: 100GB gp3
#    - Security Group: SSH (22), Custom TCP (8888 for Jupyter)
```

#### AWS CLIë¡œ ë¹ ë¥¸ ì‹œì‘

```bash
# AWS CLI ì„¤ì¹˜ ë° ì„¤ì •
aws configure

# í‚¤í˜ì–´ ìƒì„± (ìµœì´ˆ 1íšŒ)
aws ec2 create-key-pair \
  --key-name ml-training-key \
  --query 'KeyMaterial' \
  --output text > ml-training-key.pem

chmod 400 ml-training-key.pem

# Deep Learning AMI ID í™•ì¸ (us-east-1 ê¸°ì¤€)
# AMI IDëŠ” ë¦¬ì „ë³„ë¡œ ë‹¤ë¥´ë¯€ë¡œ AWS ì½˜ì†”ì—ì„œ í™•ì¸ í•„ìš”
AMI_ID="ami-0c7217cdde317cfec"  # Deep Learning AMI GPU PyTorch 2.0

# EC2 ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘
aws ec2 run-instances \
  --image-id $AMI_ID \
  --instance-type g4dn.2xlarge \
  --key-name ml-training-key \
  --block-device-mappings '[{"DeviceName":"/dev/sda1","Ebs":{"VolumeSize":100,"VolumeType":"gp3"}}]' \
  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=ml-training}]'
```

### 2ï¸âƒ£ ì¸ìŠ¤í„´ìŠ¤ ì ‘ì† ë° í™˜ê²½ ì„¤ì •

```bash
# ì¸ìŠ¤í„´ìŠ¤ Public IP í™•ì¸
aws ec2 describe-instances \
  --filters "Name=tag:Name,Values=ml-training" \
  --query 'Reservations[0].Instances[0].PublicIpAddress' \
  --output text

# SSH ì ‘ì† (IP ì£¼ì†ŒëŠ” ìœ„ì—ì„œ í™•ì¸í•œ ê²ƒìœ¼ë¡œ ëŒ€ì²´)
ssh -i ml-training-key.pem ubuntu@<YOUR_INSTANCE_IP>
```

### 3ï¸âƒ£ í”„ë¡œì íŠ¸ ì—…ë¡œë“œ ë° í•™ìŠµ

```bash
# === ë¡œì»¬ì—ì„œ ì‹¤í–‰ (í”„ë¡œì íŠ¸ ì••ì¶•) ===
cd /Users/ktg/Desktop/load_generate_ai
tar -czf pathfinding-server.tar.gz pathfinding-server/

# ì„œë²„ë¡œ ì—…ë¡œë“œ
scp -i ml-training-key.pem pathfinding-server.tar.gz ubuntu@<YOUR_INSTANCE_IP>:~/

# === EC2 ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‹¤í–‰ ===
# SSH ì ‘ì† í›„

# ì••ì¶• í•´ì œ
tar -xzf pathfinding-server.tar.gz
cd pathfinding-server

# ê°€ìƒí™˜ê²½ ìƒì„± (Deep Learning AMIëŠ” conda ì œê³µ)
conda create -n pathfinding python=3.10 -y
conda activate pathfinding

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements_ml.txt

# GPU í™•ì¸
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')"

# í•™ìŠµ ì‹œì‘ (GPU ì‚¬ìš©)
python -m app.core.ml.train \
  --generate_data \
  --epochs 50 \
  --batch_size 32 \
  --use_wandb

# ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (SSH ì—°ê²° ëŠì–´ë„ ê³„ì† ì‹¤í–‰)
nohup python -m app.core.ml.train \
  --generate_data \
  --epochs 50 \
  --batch_size 32 \
  --use_wandb > training.log 2>&1 &

# ë¡œê·¸ í™•ì¸
tail -f training.log
```

### 4ï¸âƒ£ í•™ìŠµ ëª¨ë‹ˆí„°ë§

#### TensorBoard ì›ê²© ì ‘ì†

```bash
# === EC2 ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ===
tensorboard --logdir logs --host 0.0.0.0 --port 6006 &

# === ë¡œì»¬ ì»´í“¨í„°ì—ì„œ ===
# SSH í„°ë„ë§
ssh -i ml-training-key.pem -L 6006:localhost:6006 ubuntu@<YOUR_INSTANCE_IP>

# ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
# http://localhost:6006
```

#### Weights & Biases ì‚¬ìš© (ì¶”ì²œ)

```bash
# EC2ì—ì„œ í•œ ë²ˆë§Œ ì„¤ì •
wandb login  # API key ì…ë ¥

# í•™ìŠµ ì‹œ --use_wandb í”Œë˜ê·¸ ì‚¬ìš©
python -m app.core.ml.train --generate_data --epochs 50 --use_wandb

# wandb.ai ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥
```

### 5ï¸âƒ£ í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# === EC2 ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ===
# ëª¨ë¸ ì••ì¶•
cd ~/pathfinding-server
tar -czf trained_models.tar.gz models/ logs/

# === ë¡œì»¬ ì»´í“¨í„°ì—ì„œ ===
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
scp -i ml-training-key.pem ubuntu@<YOUR_INSTANCE_IP>:~/pathfinding-server/trained_models.tar.gz .

# ì••ì¶• í•´ì œ
tar -xzf trained_models.tar.gz

# ì›ë˜ í”„ë¡œì íŠ¸ì— ë³µì‚¬
cp -r models/* /Users/ktg/Desktop/load_generate_ai/pathfinding-server/models/
cp -r logs/* /Users/ktg/Desktop/load_generate_ai/pathfinding-server/logs/
```

### 6ï¸âƒ£ ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ (ì¤‘ìš”!)

```bash
# í•™ìŠµ ì™„ë£Œ í›„ ë°˜ë“œì‹œ ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œí•˜ì—¬ ë¹„ìš© ì ˆì•½
aws ec2 stop-instances --instance-ids <INSTANCE_ID>

# ë˜ëŠ” ì™„ì „íˆ ì‚­ì œ
aws ec2 terminate-instances --instance-ids <INSTANCE_ID>
```

---

## ğŸš€ ë°©ë²• 2: AWS SageMaker (ê´€ë¦¬í˜• ì„œë¹„ìŠ¤)

### ì¥ì 
- ìë™ í™˜ê²½ ì„¤ì •
- Jupyter Notebook ì œê³µ
- ìë™ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
- ì‹¤í—˜ ì¶”ì  ê¸°ëŠ¥

### ë‹¨ì 
- EC2ë³´ë‹¤ ì•½ê°„ ë¹„ìŒˆ
- ì´ˆê¸° ì„¤ì •ì´ ë³µì¡

### 1ï¸âƒ£ SageMaker ë…¸íŠ¸ë¶ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

```bash
# AWS CLIë¡œ ìƒì„±
aws sagemaker create-notebook-instance \
  --notebook-instance-name ml-training-notebook \
  --instance-type ml.g4dn.2xlarge \
  --role-arn arn:aws:iam::<YOUR_ACCOUNT_ID>:role/SageMakerRole \
  --volume-size-in-gb 100

# ìƒíƒœ í™•ì¸
aws sagemaker describe-notebook-instance \
  --notebook-instance-name ml-training-notebook
```

### 2ï¸âƒ£ Jupyter Notebookì—ì„œ í•™ìŠµ

```python
# SageMaker Notebookì—ì„œ ì‹¤í–‰

# í”„ë¡œì íŠ¸ í´ë¡  ë˜ëŠ” ì—…ë¡œë“œ
!git clone <YOUR_REPO_URL>
# ë˜ëŠ” íŒŒì¼ ì—…ë¡œë“œ

cd pathfinding-server

# ì˜ì¡´ì„± ì„¤ì¹˜
!pip install -r requirements_ml.txt

# í•™ìŠµ ì‹¤í–‰
!python -m app.core.ml.train \
  --generate_data \
  --epochs 50 \
  --batch_size 32 \
  --use_wandb
```

### 3ï¸âƒ£ SageMaker Training Job ì‚¬ìš© (í”„ë¡œë•ì…˜)

í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ë¥¼ SageMaker Training Jobìœ¼ë¡œ ì‹¤í–‰í•˜ë©´:
- ìë™ ìŠ¤ì¼€ì¼ë§
- ë¶„ì‚° í•™ìŠµ ì§€ì›
- S3 í†µí•©

```python
# sagemaker_training.py ì˜ˆì œ
import sagemaker
from sagemaker.pytorch import PyTorch

# SageMaker Session
session = sagemaker.Session()
role = sagemaker.get_execution_role()

# PyTorch Estimator ìƒì„±
estimator = PyTorch(
    entry_point='app/core/ml/train.py',
    source_dir='.',
    role=role,
    framework_version='2.0',
    py_version='py310',
    instance_count=1,
    instance_type='ml.g4dn.2xlarge',
    hyperparameters={
        'epochs': 50,
        'batch_size': 32,
        'lr': 0.001,
        'generate_data': True
    },
    output_path='s3://your-bucket/models',
    base_job_name='map-segmentation'
)

# í•™ìŠµ ì‹œì‘
estimator.fit()
```

---

## ğŸ’¡ ë¹„ìš© ì ˆì•½ íŒ

### 1. Spot Instances ì‚¬ìš© (ìµœëŒ€ 90% í• ì¸)

```bash
# EC2 Spot Instance ìš”ì²­
aws ec2 request-spot-instances \
  --spot-price "0.3" \
  --instance-count 1 \
  --type "one-time" \
  --launch-specification '{
    "ImageId": "ami-0c7217cdde317cfec",
    "InstanceType": "g4dn.2xlarge",
    "KeyName": "ml-training-key",
    "BlockDeviceMappings": [{
      "DeviceName": "/dev/sda1",
      "Ebs": {"VolumeSize": 100, "VolumeType": "gp3"}
    }]
  }'
```

**ì£¼ì˜**: Spot InstanceëŠ” ì¤‘ë‹¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í•„ìˆ˜

### 2. í•™ìŠµ ì™„ë£Œ í›„ ìë™ ì¢…ë£Œ

```bash
# EC2 ì¸ìŠ¤í„´ìŠ¤ì—ì„œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ëì— ì¶”ê°€
python -m app.core.ml.train --generate_data --epochs 50

# í•™ìŠµ ì™„ë£Œ í›„ ìë™ ì¢…ë£Œ
sudo shutdown -h now
```

### 3. ì˜ˆìƒ ë¹„ìš© ê³„ì‚°

| í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ | ì¸ìŠ¤í„´ìŠ¤ | ì˜ˆìƒ ì‹œê°„ | ë¹„ìš© |
|--------------|---------|----------|------|
| 50 epochs, batch 16 | g4dn.2xlarge | ~3ì‹œê°„ | $2.25 |
| 100 epochs, batch 32 | g4dn.2xlarge | ~5ì‹œê°„ | $3.76 |
| 50 epochs, batch 32 | g5.xlarge | ~2ì‹œê°„ | $2.01 |

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### GPU ì¸ì‹ ì•ˆë  ë•Œ

```bash
# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# CUDA ë²„ì „ í™•ì¸
nvcc --version

# PyTorch CUDA í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"
```

### Out of Memory ì—ëŸ¬

```python
# train.pyì—ì„œ batch_size ì¤„ì´ê¸°
python -m app.core.ml.train \
  --generate_data \
  --epochs 50 \
  --batch_size 8  # 32 â†’ 8
```

### SSH ì—°ê²° ëŠê¹€

```bash
# tmux ì‚¬ìš© (ì„¸ì…˜ ìœ ì§€)
tmux new -s training

# í•™ìŠµ ì‹¤í–‰
python -m app.core.ml.train --generate_data --epochs 50

# Ctrl+B, Dë¡œ detach
# ë‚˜ì¤‘ì— ì¬ì ‘ì†
tmux attach -t training
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| í™˜ê²½ | GPU | ë°°ì¹˜ í¬ê¸° | Epochë‹¹ ì‹œê°„ | 50 Epochs ì´ ì‹œê°„ |
|-----|-----|----------|------------|-----------------|
| **M1 Mac** | Apple Silicon | 8 | ~8ë¶„ | ~6.5ì‹œê°„ |
| **EC2 g4dn.2xlarge** | NVIDIA T4 | 32 | ~2ë¶„ | ~1.7ì‹œê°„ |
| **EC2 g5.xlarge** | NVIDIA A10G | 32 | ~1.5ë¶„ | ~1.2ì‹œê°„ |
| **EC2 p3.2xlarge** | NVIDIA V100 | 32 | ~1ë¶„ | ~50ë¶„ |

---

## âœ… ê¶Œì¥ ì›Œí¬í”Œë¡œìš°

1. **ë¡œì»¬ (M1)**: ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ (5 epochs, ì‘ì€ ë°ì´í„°ì…‹)
2. **AWS EC2 g4dn.2xlarge**: ì „ì²´ í•™ìŠµ (50-100 epochs)
3. **ê²°ê³¼ ë‹¤ìš´ë¡œë“œ**: ëª¨ë¸ì„ ë¡œì»¬ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©

```bash
# ë¡œì»¬ í…ŒìŠ¤íŠ¸
python -m app.core.ml.train --generate_data --epochs 5 --batch_size 4

# AWSì—ì„œ ì „ì²´ í•™ìŠµ
# (ìœ„ ê°€ì´ë“œ ì°¸ê³ )

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í›„ ë¡œì»¬ì—ì„œ ì‚¬ìš©
python -m app.core.ml.train --model_path models/best_model.pth
```
