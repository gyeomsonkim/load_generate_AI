# ML ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„ë¦¬ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

## ğŸ“‹ ë³€ê²½ ì‚¬í•­

### ì•„í‚¤í…ì²˜ ë³€í™”

**Before (Monolith)**:
```
pathfinding-server/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ ml/  â† ML ì½”ë“œ ë‚´ì¥
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ ml_service.py  â† ì§ì ‘ ëª¨ë¸ í˜¸ì¶œ
```

**After (Microservice)**:
```
ml-inference-server/        pathfinding-server/
â”œâ”€â”€ app/                    â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ml/  â† ML ì½”ë“œ       â”‚   â””â”€â”€ services/
â”‚   â””â”€â”€ api/                 â”‚       â”œâ”€â”€ ml_client.py  â† HTTP Client
      â””â”€â”€ inference.py       â”‚       â””â”€â”€ ml_service.py  â† HTTP ìš”ì²­
```

---

## ğŸš€ ë§ˆì´ê·¸ë ˆì´ì…˜ ë‹¨ê³„

### 1ë‹¨ê³„: ml-inference-server ì„¤ì •

```bash
cd /Users/ktg/Desktop/load_generate_ai/ml-inference-server

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# .env í™•ì¸
cat .env
# ML_DEVICE=cpu
# MODEL_TYPE=unet
# PORT=8001

# ì„œë²„ ì‹¤í–‰ (í„°ë¯¸ë„ 1)
uvicorn app.main:app --port 8001 --reload
```

**í™•ì¸**:
```bash
curl http://localhost:8001/health
# ì˜ˆìƒ ì¶œë ¥: {"status":"degraded","model_loaded":false,...}
# â†’ ì •ìƒ (ëª¨ë¸ì€ ì•„ì§ í•™ìŠµ ì•ˆ ë¨)
```

---

### 2ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ (ì„ íƒì‚¬í•­)

ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ì„¸ìš”.

```bash
cd /Users/ktg/Desktop/load_generate_ai/ml-inference-server

# í•©ì„± ë°ì´í„° ìƒì„± + í•™ìŠµ
python -m app.ml.train --generate_data --epochs 20 --batch_size 4

# í•™ìŠµ ì™„ë£Œ í›„ ì„œë²„ ì¬ì‹œì‘
# í„°ë¯¸ë„ 1ì—ì„œ Ctrl+C í›„
uvicorn app.main:app --port 8001 --reload
```

**í™•ì¸**:
```bash
curl http://localhost:8001/health
# ì˜ˆìƒ ì¶œë ¥: {"status":"healthy","model_loaded":true,...}
```

---

### 3ë‹¨ê³„: pathfinding-server ì„¤ì •

```bash
cd /Users/ktg/Desktop/load_generate_ai/pathfinding-server

# ì˜ì¡´ì„± ì¬ì„¤ì¹˜ (ML ì œê±°, httpx ì¶”ê°€)
pip install -r requirements.txt

# .env í™•ì¸
cat .env | grep ML_INFERENCE
# ML_INFERENCE_URL=http://localhost:8001

# ì„œë²„ ì‹¤í–‰ (í„°ë¯¸ë„ 2)
uvicorn app.main:app --port 8000 --reload
```

---

### 4ë‹¨ê³„: í†µí•© í…ŒìŠ¤íŠ¸

```bash
# pathfinding-serverê°€ ML ì„œë²„ë¥¼ í˜¸ì¶œí•˜ëŠ”ì§€ í™•ì¸
curl http://localhost:8000/api/v1/ml/status

# ì˜ˆìƒ ì¶œë ¥:
# {
#   "ml_enabled": true,
#   "model_info": {
#     "model_loaded": true,  â† ML ì„œë²„ ì—°ê²° ì„±ê³µ
#     ...
#   }
# }
```

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: "ML server health check failed"

**ì›ì¸**: ml-inference-serverê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹˜

**í•´ê²°**:
```bash
# í„°ë¯¸ë„ 1ì—ì„œ
cd ml-inference-server
uvicorn app.main:app --port 8001
```

---

### ë¬¸ì œ 2: "Model not loaded"

**ì›ì¸**: í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŒ

**í•´ê²° 1**: ë¡œì»¬ì—ì„œ í•™ìŠµ
```bash
cd ml-inference-server
python -m app.ml.train --generate_data --epochs 20 --batch_size 4
```

**í•´ê²° 2**: EC2ì—ì„œ í•™ìŠµ í›„ ë‹¤ìš´ë¡œë“œ
```bash
# EC2ì—ì„œ í•™ìŠµ í›„
scp -i key.pem -r ubuntu@ec2-ip:~/ml-inference-server/models ./
```

---

### ë¬¸ì œ 3: "Connection refused"

**ì›ì¸**: í¬íŠ¸ ì¶©ëŒ ë˜ëŠ” ë°©í™”ë²½

**í•´ê²°**:
```bash
# í¬íŠ¸ ì‚¬ìš© í™•ì¸
lsof -i :8001
lsof -i :8000

# ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
uvicorn app.main:app --port 8002
# .env ìˆ˜ì •: ML_INFERENCE_URL=http://localhost:8002
```

---

### ë¬¸ì œ 4: Import ì—ëŸ¬

**ì›ì¸**: ì˜ì¡´ì„± ëˆ„ë½

**í•´ê²°**:
```bash
# pathfinding-server
pip install httpx tenacity

# ml-inference-server
pip install -r requirements.txt
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| ë©”íŠ¸ë¦­ | Before (Monolith) | After (Microservice) |
|--------|------------------|---------------------|
| ì¶”ë¡  ì‹œê°„ | 1.8s | 1.9s (+0.1s HTTP) |
| ë©”ëª¨ë¦¬ (pathfinding) | 2.5GB | 500MB (-2GB) |
| ë©”ëª¨ë¦¬ (ML ì„œë²„) | - | 2GB |
| ë°°í¬ ë…ë¦½ì„± | âŒ | âœ… |
| í™•ì¥ì„± | â­â­ | â­â­â­â­â­ |

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### ë¡œì»¬ ê°œë°œ í™˜ê²½
```bash
# í„°ë¯¸ë„ 1: ML ì„œë²„
cd ml-inference-server
uvicorn app.main:app --port 8001 --reload

# í„°ë¯¸ë„ 2: ë©”ì¸ ì„œë²„
cd pathfinding-server
uvicorn app.main:app --port 8000 --reload
```

### EC2 ë°°í¬
```bash
# ML ì„œë²„ ë°°í¬
scp -r ml-inference-server ubuntu@ec2-ml:~/
ssh ubuntu@ec2-ml
cd ml-inference-server
uvicorn app.main:app --host 0.0.0.0 --port 8001

# ë©”ì¸ ì„œë²„ .env ìˆ˜ì •
ML_INFERENCE_URL=http://ec2-ml-ip:8001
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ml-inference-serverê°€ 8001 í¬íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘
- [ ] /health ì—”ë“œí¬ì¸íŠ¸ ì •ìƒ ì‘ë‹µ
- [ ] ëª¨ë¸ì´ ë¡œë“œë¨ (ë˜ëŠ” í•™ìŠµ ì™„ë£Œ)
- [ ] pathfinding-serverê°€ 8000 í¬íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘
- [ ] pathfinding-serverê°€ ML ì„œë²„ì— ì—°ê²° ê°€ëŠ¥
- [ ] /api/v1/ml/statusì—ì„œ ML ì„œë²„ ìƒíƒœ í™•ì¸ ê°€ëŠ¥
- [ ] ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° ì„¸ê·¸ë©˜í…Œì´ì…˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ

---

## ğŸ’¡ FAQ

**Q: ê¸°ì¡´ ì½”ë“œì—ì„œ ML ê´€ë ¨ import ì—ëŸ¬ê°€ ë‚˜ìš”**
A: `app/core/ml/` ë””ë ‰í† ë¦¬ê°€ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ í•´ë‹¹ importë¥¼ ì œê±°í•˜ê±°ë‚˜ HTTP Clientë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½í•˜ì„¸ìš”.

**Q: ë¡œì»¬ì—ì„œë§Œ ê°œë°œí•  ë•Œë„ ë‘ ì„œë²„ë¥¼ ë„ì›Œì•¼ í•˜ë‚˜ìš”?**
A: ë„¤. í•˜ì§€ë§Œ tmuxë‚˜ í„°ë¯¸ë„ ì—¬ëŸ¬ ê°œë¡œ ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Q: ML ì„œë²„ê°€ ë‹¤ìš´ë˜ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?**
A: `ENABLE_ML_FALLBACK=true`ë¡œ ì„¤ì •í•˜ë©´ ìë™ìœ¼ë¡œ CV ë°©ì‹ìœ¼ë¡œ Fallbackë©ë‹ˆë‹¤.

**Q: ë¹„ìš©ì´ ë” ë“œë‚˜ìš”?**
A: ë¡œì»¬ ê°œë°œì—ì„œëŠ” ë™ì¼. EC2 ë°°í¬ ì‹œ ML ì„œë²„ìš© ì¸ìŠ¤í„´ìŠ¤ê°€ ì¶”ê°€ë¡œ í•„ìš”í•˜ì§€ë§Œ, í•„ìš”í•  ë•Œë§Œ ì¼œë©´ ë©ë‹ˆë‹¤.

**Q: ë‹¤ì‹œ Monolithë¡œ ëŒì•„ê°ˆ ìˆ˜ ìˆë‚˜ìš”?**
A: Gitìœ¼ë¡œ ì´ì „ ì»¤ë°‹ìœ¼ë¡œ ë¡¤ë°±í•˜ë©´ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ê°€ ë” ìœ ì—°í•˜ê³  í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.
